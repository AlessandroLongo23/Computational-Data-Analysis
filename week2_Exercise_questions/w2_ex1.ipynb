{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To embed plots in the notebooks\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np # numpy library\n",
    "import scipy.linalg as lng # linear algebra from scipy library\n",
    "from sklearn import preprocessing as preproc # load preprocessing function\n",
    "\n",
    "# seaborn can be used to \"prettify\" default matplotlib plots by importing and setting as default\n",
    "import seaborn as sns\n",
    "sns.set() # Set searborn as default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prostatePath = 'Prostate.txt'\n",
    "\n",
    "T = np.loadtxt(prostatePath, delimiter = ' ', skiprows = 1, usecols=[1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "y = T[:, 0]\n",
    "X = T[:,1:]\n",
    "\n",
    "[n, p] = np.shape(X)\n",
    "\n",
    "#Our chosen normalization centers and normalize the variables of a data matrix to unit length.\n",
    "# We can use sklearn \"Normalizer\" to do this, but we must transpose the matrices to act on the variables instead of samples\n",
    "X_pre = X - np.mean(X,axis=0)\n",
    "y_pre = y - np.mean(y,axis=0)\n",
    "\n",
    "normalizer = preproc.Normalizer().fit(X_pre.T)  \n",
    "X_pre = normalizer.transform(X_pre.T).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Perform model selection for ridge regression (for the prostate data set):\n",
    "> (a) Consider using ridge-regression solutions for the prostate data set. What is a suitable range for the shrinkage parameter $λ$ in which to search for an optimal solution in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridgeMulti(X, _lambda, p, y):\n",
    "    \"\"\"Ridge implementation from last week\"\"\"\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100; # try k values of lambda\n",
    "lambdas = np.logspace(-4, 4, k)\n",
    "\n",
    "betas = np.zeros((p,k))\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "plt.figure()\n",
    "plt.semilogx(lambdas, betas.T )\n",
    "plt.xlabel(\"Lambdas\")\n",
    "plt.ylabel(\"Betas\")\n",
    "plt.suptitle(\"Regularized beta estimates\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a suitable range for $λ$ in which to search for an optimal solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) Select a suitable value for the regularization parameter using K-fold cross-validation. Plot the resulting optimal value of lambda on a plot of the parameter trace (i.e. a plot of the $\\hat{β}_j$s as a function of $λ$).\n",
    "\n",
    ">> (i) Try one of the common choices K = 5 and K = 10, and run the cross- validation a couple of times. Which would you prefer and why?\n",
    "*Hint: To do Crossvalidation create a vector of length n that contains equal amounts of numbers from 1 to K and permute that vector.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centerData(data):\n",
    "    \n",
    "    mu = np.mean(data,axis=0)\n",
    "    data = data - mu\n",
    "    \n",
    "    return data, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "lambdas = np.logspace(-4, 4, k)\n",
    "MSE = np.zeros((10, 100))\n",
    "\n",
    "#Create a vector of length n that contains equal amounts of numbers from 1 to K\n",
    "\n",
    "#Permute that vector. \n",
    "\n",
    "\n",
    "# For each chunk of data; run ridge for each lambda and calculate mean squared error\n",
    "for i in range(1, K+1):\n",
    "\n",
    "# average the mean squared error across chunks for the same lambda values to find the optimal lambda    \n",
    "\n",
    "# Remember excact solution depends on a random indexing, so results may vary\n",
    "# I reuse the plot with all the betas from 1 a) and add a line for the optimal value of lambda\n",
    "plt.figure()\n",
    "plt.semilogx(lambdas, betas.T )\n",
    "plt.xlabel(\"Lambdas\")\n",
    "plt.ylabel(\"Betas\")\n",
    "plt.suptitle(f\"Optimal lambda: {lambda_OP}\", fontsize=20)\n",
    "plt.semilogx([lambda_OP, lambda_OP], [np.min(betas), np.max(betas)], marker = \".\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where should you normalize your data?\n",
    "\n",
    "(ii) What is the value of K corresponding to leave-one-out cross-validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (c) Find a suitable value of $λ$ using the one-standard-error rule. What is the difference between the two strategies (cross-validation and cross-validation with one- standard-error-rule)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard error for the best lambda, and find a the largest lambda with a MSE that is within\n",
    "# the range of the optimal lambda +- the standard error.\n",
    "\n",
    "Lambda_CV_1StdRule = \n",
    "print(\"CV lambda with 1-std-rule %0.2f\" % Lambda_CV_1StdRule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (d) Select suitable values for the regularization parameter using the AIC and BIC criteria (cf. 7.5-7.7 in ESL). What are the advantages and disadvantages of using cross-validation vs. information criteria?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.zeros(100)\n",
    "AIC = np.zeros(100)\n",
    "BIC = np.zeros(100)\n",
    "\n",
    "# calculate the AIC and BIC for ridge regression for different lambdas\n",
    "# pick the lambda based on themodels with the lowest AIC and BIC\n",
    "\n",
    "jAIC = \n",
    "jBIC =\n",
    "\n",
    "print(\"AIC at %0.2f\" % lambdas[jAIC])\n",
    "print(\"BIC at %0.2f\" % lambdas[jBIC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot different methods Information criteria: AIC BIC CV\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "axs[0].set_title('Information criteria')\n",
    "_ = axs[0].semilogx(lambdas,AIC,'-r',label='AIC')\n",
    "axs[0].semilogx(lambdas,BIC/300,'-b',label='BIC')\n",
    "axs[0].semilogx(lambdas,meanMSE,'-k',label='CV')\n",
    "axs[0].semilogx(lambdas[jAIC],np.min(AIC),'*r',label='AIC')\n",
    "axs[0].semilogx(lambdas[jBIC],np.min(BIC)/300,'*b',label='BIC')\n",
    "axs[0].semilogx(lambdas[jOpt],np.min(meanMSE),'*k',label='CV')\n",
    "axs[0].legend()\n",
    "axs[0].set_xlabel('Lambda')\n",
    "axs[0].set_ylabel('Criteria')\n",
    "\n",
    "#plot the degree of freedom\n",
    "axs[1].semilogx(lambdas,D,'-k');\n",
    "axs[1].set_title('Degrees of freedom');\n",
    "axs[1].set_xlabel('Lambda');\n",
    "axs[1].set_ylabel('d');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (e) Use the bootstrap to estimate the variance of the parameters of the solution $(β)$ for each value of lambda in exercise 1a. Plot the variance estimates as a function of lambda. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBoot = 100\n",
    "Beta = np.zeros((p, len(lambdas), NBoot))\n",
    "\n",
    "# Run bootstrap on ridge for the same range of lambdas as before\n",
    "# similarly as with cross validation you run the bootstrap x times and each time you sample with replacement\n",
    "# ridge is then run on that sample for each lambda value\n",
    "\n",
    "# Plot the varriance of the betas for each lambda value\n",
    "plt.figure()\n",
    "for i in range(8):\n",
    "    plt.semilogx(lambdas, stdBeta[i,:])\n",
    "plt.title(\"Bootstrapped variance\")\n",
    "plt.ylabel(\"beta\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
